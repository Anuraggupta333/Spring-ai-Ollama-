# Spring Application Name
spring.application.name=Ollama-ai

# Ollama Local Server URL
spring.ai.ollama.url=http://localhost:11434

# Model Name for Ollama (you need to set this)
spring.ai.ollama.chat.options.model=mistral

# Temperature for AI responses (ensure it's a valid number, e.g., 0.7)
spring.ai.ollama.chat.options.temperature=0.2
